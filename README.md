# Diligent Assignment – E-commerce Data Pipeline

This project sets up a simple e-commerce data pipeline with synthetic CSV datasets, ingestion into a local SQLite database, and sample analytical SQL queries.

## What’s included

- Folder structure under `ecommerce-data-pipeline/`:
  - `data/` – CSV datasets (products, customers, orders, order_items, reviews)
  - `scripts/` – Python scripts for ingestion and running queries
  - `sql/` – SQL query file (`queries.sql`)
  - `ecommerce.db` – SQLite database generated by the ingestion script
- Root-level `prompts_used.txt` – a log of the prompts used to create folders, generate data, write the ingestion script, and create queries

## How this was created

- Folders (`data`, `scripts`, `sql`) were created inside `ecommerce-data-pipeline/`.
- Synthetic CSV data was generated to populate the `data/` directory.
- A Python ingestion script (`scripts/ingest_sqlite.py`) loads the CSVs into `ecommerce.db` using pandas and sqlite3, and creates helpful indexes for joins.
- A Python query runner (`scripts/run_queries.py`) executes the SQL in `sql/queries.sql` without requiring the sqlite3 CLI.

All the prompts we used to drive these steps are documented in `prompts_used.txt`.

## Quick start

1. Ingest data into SQLite (creates/overwrites `ecommerce.db`):
   ```powershell
   python ".\ecommerce-data-pipeline\scripts\ingest_sqlite.py"
   ```

2. Run the queries and print CSV results to the console:
   ```powershell
   python ".\ecommerce-data-pipeline\scripts\run_queries.py"
   ```

## Notes

- The SQL queries consider both `completed` and `delivered` statuses to match the dataset.
- You can edit or add queries in `ecommerce-data-pipeline/sql/queries.sql` and re-run the query runner.
